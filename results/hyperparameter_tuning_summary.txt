======================================================================
HYPERPARAMETER TUNING SUMMARY
======================================================================

Date: 2026-01-21 09:31:28
Dataset size: 9998 rows
Train set: 7998 samples
Test set: 2000 samples

Label Encoding:
  blocker -> 0
  critical -> 1
  major -> 2
  minor -> 3
  normal -> 4
  trivial -> 5

----------------------------------------------------------------------
MODEL COMPARISON
----------------------------------------------------------------------

Random Forest:
  Best CV F1-Macro: 0.3373
  Tuning Time: 13.31 minutes
  Best Parameters:
    classifier__max_depth: 20
    classifier__min_samples_split: 2
    classifier__n_estimators: 200
    feature_combiner__ngram_range: (1, 1)

Logistic Regression:
  Best CV F1-Macro: 0.2876
  Tuning Time: 2.63 minutes
  Best Parameters:
    classifier__C: 1
    classifier__penalty: l2
    feature_combiner__ngram_range: (1, 2)

======================================================================
BEST MODEL: Random Forest
CV F1-Macro: 0.3373
Test F1-Macro: 0.2972
======================================================================

TEST SET PERFORMANCE:
              precision    recall  f1-score   support

     blocker       0.14      0.05      0.07        41
    critical       0.49      0.48      0.49       121
       major       0.18      0.22      0.20       148
       minor       0.10      0.06      0.07       108
      normal       0.81      0.81      0.81      1521
     trivial       0.11      0.21      0.15        61

    accuracy                           0.67      2000
   macro avg       0.31      0.30      0.30      2000
weighted avg       0.67      0.67      0.67      2000
