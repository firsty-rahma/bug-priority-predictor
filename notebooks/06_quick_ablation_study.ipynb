{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefcd2da-1e12-49fd-9acd-f7f8d6d8d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ad9a33-f66f-4867-9ffb-db127f2e7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCombiner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Combine TF-IDF text features with categorical features\"\"\"\n",
    "    \n",
    "    def __init__(self, max_features=1000, min_df=2, max_df=0.8, \n",
    "                 ngram_range=(1, 1), remove_crash=False):\n",
    "        self.max_features = max_features\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.remove_crash = remove_crash\n",
    "        self.tfidf = None\n",
    "        self.encoder_component = None\n",
    "        self.encoder_product = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Build stopwords list\n",
    "        if self.remove_crash:\n",
    "            from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "            # Add crash and its variants to stopwords\n",
    "            crash_variants = ['crash', 'crashes', 'crashed', 'crashing', 'crasher']\n",
    "            stop_words = list(ENGLISH_STOP_WORDS) + crash_variants\n",
    "            print(f\"  âš ï¸  Removing from vocabulary: {crash_variants}\")\n",
    "        else:\n",
    "            stop_words = 'english'\n",
    "        \n",
    "        # TF-IDF for text\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            max_features=self.max_features,\n",
    "            min_df=self.min_df,\n",
    "            max_df=self.max_df,\n",
    "            ngram_range=self.ngram_range,\n",
    "            stop_words=stop_words\n",
    "        )\n",
    "        self.tfidf.fit(X['text_processed'])\n",
    "        \n",
    "        # OrdinalEncoder for categorical\n",
    "        self.encoder_component = OrdinalEncoder(\n",
    "            handle_unknown='use_encoded_value',\n",
    "            unknown_value=-1\n",
    "        )\n",
    "        self.encoder_component.fit(X[['component_name']])\n",
    "        \n",
    "        self.encoder_product = OrdinalEncoder(\n",
    "            handle_unknown='use_encoded_value',\n",
    "            unknown_value=-1\n",
    "        )\n",
    "        self.encoder_product.fit(X[['product_name']])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_tfidf = self.tfidf.transform(X['text_processed'])\n",
    "        component_encoded = self.encoder_component.transform(X[['component_name']])\n",
    "        product_encoded = self.encoder_product.transform(X[['product_name']])\n",
    "        text_length = X['text_length'].values.reshape(-1, 1)\n",
    "        X_combined = hstack([X_tfidf, component_encoded, product_encoded, text_length])\n",
    "        return X_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da792d16-d5f1-4d0a-9802-36ce940b86eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: 9998 samples\n",
      "Dev: 7998, Test: 2000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/bugs_preprocessed.csv')\n",
    "data = data[\n",
    "    data['text_processed'].notna() & \n",
    "    (data['text_processed'].str.strip() != '')\n",
    "].copy().reset_index(drop=True)\n",
    "data['text_length'] = data['text_processed'].str.split().str.len()\n",
    "\n",
    "X_combined_data = pd.DataFrame({\n",
    "    'text_processed': data['text_processed'],\n",
    "    'component_name': data['component_name'],\n",
    "    'product_name': data['product_name'],\n",
    "    'text_length': data['text_length']\n",
    "})\n",
    "y_original = data['severity_category']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_original)\n",
    "\n",
    "# Split\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_combined_data, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset: {len(data)} samples\")\n",
    "print(f\"Dev: {len(X_dev)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7834a3a-e3a2-465f-a851-065d6f784b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPERIMENT 1: BASELINE (WITH 'crash')\n",
      "======================================================================\n",
      "Training baseline...\n",
      "\n",
      "âœ… Baseline Results:\n",
      "   F1-Macro:    0.3010\n",
      "   F1-Weighted: 0.6768\n",
      "\n",
      "   Per-class F1:\n",
      "     blocker     : 0.0769\n",
      "     critical    : 0.4856\n",
      "     major       : 0.1977\n",
      "     minor       : 0.0952\n",
      "     normal      : 0.8179\n",
      "     trivial     : 0.1325\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 1: BASELINE (WITH 'crash')\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_pipeline = ImbPipeline([\n",
    "    ('feature_combiner', FeatureCombiner(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2),\n",
    "        remove_crash=False  # KEEP crash\n",
    "    )),\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=3)),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Training baseline...\")\n",
    "baseline_pipeline.fit(X_dev, y_dev)\n",
    "\n",
    "y_pred_baseline = baseline_pipeline.predict(X_test)\n",
    "y_pred_baseline_labels = label_encoder.inverse_transform(y_pred_baseline)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "baseline_f1_macro = f1_score(y_test_labels, y_pred_baseline_labels, average='macro')\n",
    "baseline_f1_weighted = f1_score(y_test_labels, y_pred_baseline_labels, average='weighted')\n",
    "baseline_f1_per_class = f1_score(y_test_labels, y_pred_baseline_labels, \n",
    "                                  average=None, labels=sorted(label_encoder.classes_))\n",
    "\n",
    "print(f\"\\nâœ… Baseline Results:\")\n",
    "print(f\"   F1-Macro:    {baseline_f1_macro:.4f}\")\n",
    "print(f\"   F1-Weighted: {baseline_f1_weighted:.4f}\")\n",
    "print(\"\\n   Per-class F1:\")\n",
    "for idx, cls in enumerate(sorted(label_encoder.classes_)):\n",
    "    print(f\"     {cls:12s}: {baseline_f1_per_class[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e7bb53-1590-4b56-907b-b7b2a9177410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPERIMENT 2: ABLATION (WITHOUT 'crash')\n",
      "======================================================================\n",
      "Training ablation model...\n",
      "  âš ï¸  Removing from vocabulary: ['crash', 'crashes', 'crashed', 'crashing', 'crasher']\n",
      "\n",
      "âœ… Ablation Results:\n",
      "   F1-Macro:    0.2748\n",
      "   F1-Weighted: 0.6597\n",
      "\n",
      "   Per-class F1:\n",
      "     blocker     : 0.0741\n",
      "     critical    : 0.3276\n",
      "     major       : 0.1697\n",
      "     minor       : 0.1163\n",
      "     normal      : 0.8085\n",
      "     trivial     : 0.1529\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2: ABLATION (WITHOUT 'crash')\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ablation_pipeline = ImbPipeline([\n",
    "    ('feature_combiner', FeatureCombiner(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2),\n",
    "        remove_crash=True  # REMOVE crash\n",
    "    )),\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=3)),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Training ablation model...\")\n",
    "ablation_pipeline.fit(X_dev, y_dev)\n",
    "\n",
    "y_pred_ablation = ablation_pipeline.predict(X_test)\n",
    "y_pred_ablation_labels = label_encoder.inverse_transform(y_pred_ablation)\n",
    "\n",
    "ablation_f1_macro = f1_score(y_test_labels, y_pred_ablation_labels, average='macro')\n",
    "ablation_f1_weighted = f1_score(y_test_labels, y_pred_ablation_labels, average='weighted')\n",
    "ablation_f1_per_class = f1_score(y_test_labels, y_pred_ablation_labels, \n",
    "                                  average=None, labels=sorted(label_encoder.classes_))\n",
    "\n",
    "print(f\"\\nâœ… Ablation Results:\")\n",
    "print(f\"   F1-Macro:    {ablation_f1_macro:.4f}\")\n",
    "print(f\"   F1-Weighted: {ablation_f1_weighted:.4f}\")\n",
    "print(\"\\n   Per-class F1:\")\n",
    "for idx, cls in enumerate(sorted(label_encoder.classes_)):\n",
    "    print(f\"     {cls:12s}: {ablation_f1_per_class[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7910db74-a24b-4b2a-89f4-7fcb10a354c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“Š COMPARISON: IMPACT OF REMOVING 'CRASH'\n",
      "======================================================================\n",
      "\n",
      "Overall Metrics:\n",
      "Metric               Baseline     Ablation     Î” Absolute      Î” Percent   \n",
      "---------------------------------------------------------------------------\n",
      "F1-Macro             0.3010       0.2748       0.0261+++++++++ 8.68++++++++%\n",
      "F1-Weighted          0.6768       0.6597       0.0171+++++++++ 2.52++++++++%\n",
      "\n",
      "\n",
      "Per-Class Impact:\n",
      "Class           Baseline     Ablation     Î” Absolute      Î” Percent   \n",
      "---------------------------------------------------------------------------\n",
      "blocker         0.0769       0.0741       0.0028+++++++++ 3.70++++++++%\n",
      "critical        0.4856       0.3276       0.1580+++++++++ 32.54+++++++%\n",
      "major           0.1977       0.1697       0.0280+++++++++ 14.15+++++++%\n",
      "minor           0.0952       0.1163       -0.0210++++++++ -22.09++++++%\n",
      "normal          0.8179       0.8085       0.0094+++++++++ 1.15++++++++%\n",
      "trivial         0.1325       0.1529       -0.0204++++++++ -15.41++++++%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š COMPARISON: IMPACT OF REMOVING 'CRASH'\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "f1_macro_diff = baseline_f1_macro - ablation_f1_macro\n",
    "f1_weighted_diff = baseline_f1_weighted - ablation_f1_weighted\n",
    "f1_macro_pct = (f1_macro_diff / baseline_f1_macro) * 100 if baseline_f1_macro > 0 else 0\n",
    "f1_weighted_pct = (f1_weighted_diff / baseline_f1_weighted) * 100 if baseline_f1_weighted > 0 else 0\n",
    "\n",
    "print(\"\\nOverall Metrics:\")\n",
    "print(f\"{'Metric':<20} {'Baseline':<12} {'Ablation':<12} {'Î” Absolute':<15} {'Î” Percent':<12}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"{'F1-Macro':<20} {baseline_f1_macro:<12.4f} {ablation_f1_macro:<12.4f} \"\n",
    "      f\"{f1_macro_diff:+<15.4f} {f1_macro_pct:+<12.2f}%\")\n",
    "print(f\"{'F1-Weighted':<20} {baseline_f1_weighted:<12.4f} {ablation_f1_weighted:<12.4f} \"\n",
    "      f\"{f1_weighted_diff:+<15.4f} {f1_weighted_pct:+<12.2f}%\")\n",
    "\n",
    "print(\"\\n\\nPer-Class Impact:\")\n",
    "print(f\"{'Class':<15} {'Baseline':<12} {'Ablation':<12} {'Î” Absolute':<15} {'Î” Percent':<12}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for idx, cls in enumerate(sorted(label_encoder.classes_)):\n",
    "    base_f1 = baseline_f1_per_class[idx]\n",
    "    abl_f1 = ablation_f1_per_class[idx]\n",
    "    diff = base_f1 - abl_f1\n",
    "    pct = (diff / base_f1 * 100) if base_f1 > 0 else 0\n",
    "    print(f\"{cls:<15} {base_f1:<12.4f} {abl_f1:<12.4f} {diff:+<15.4f} {pct:+<12.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750a3047-f430-4eab-8b75-e4393b9a4f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“‹ SUMMARY\n",
      "======================================================================\n",
      "\n",
      "âœ… HYPOTHESIS CONFIRMED:\n",
      "   Removing 'crash' caused 8.7% performance drop in F1-Macro\n",
      "   This validates that 'crash' is a CRITICAL feature for severity detection\n",
      "\n",
      "   Most impacted class: critical\n",
      "   F1-score drop: 0.1580\n",
      "\n",
      "======================================================================\n",
      "âœ… ABLATION STUDY COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "ABLATION STUDY SUMMARY\n",
      "======================\n",
      "\n",
      "Hypothesis: Removing 'crash' (most important feature) should degrade performance\n",
      "\n",
      "Results:\n",
      "- Baseline F1-Macro:    0.3010\n",
      "- Ablation F1-Macro:    0.2748\n",
      "- Difference:           +0.0261 (+8.68%)\n",
      "\n",
      "- Baseline F1-Weighted: 0.6768\n",
      "- Ablation F1-Weighted: 0.6597\n",
      "- Difference:           +0.0171 (+2.52%)\n",
      "\n",
      "Conclusion: CONFIRMED - crash is critical\n",
      "\n",
      "ðŸ“„ Summary saved: ../results/ablation_study_quick_summary.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“‹ SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if f1_macro_diff > 0.01:  # Meaningful drop\n",
    "    print(f\"\\nâœ… HYPOTHESIS CONFIRMED:\")\n",
    "    print(f\"   Removing 'crash' caused {abs(f1_macro_pct):.1f}% performance drop in F1-Macro\")\n",
    "    print(f\"   This validates that 'crash' is a CRITICAL feature for severity detection\")\n",
    "    \n",
    "    # Find most impacted class\n",
    "    per_class_diffs = baseline_f1_per_class - ablation_f1_per_class\n",
    "    most_impacted_idx = np.argmax(per_class_diffs)\n",
    "    most_impacted_class = sorted(label_encoder.classes_)[most_impacted_idx]\n",
    "    most_impacted_drop = per_class_diffs[most_impacted_idx]\n",
    "    \n",
    "    print(f\"\\n   Most impacted class: {most_impacted_class}\")\n",
    "    print(f\"   F1-score drop: {most_impacted_drop:.4f}\")\n",
    "    \n",
    "elif f1_macro_diff < -0.01:  # Improved without crash\n",
    "    print(f\"\\nâš ï¸  UNEXPECTED RESULT:\")\n",
    "    print(f\"   Model performed BETTER without 'crash' ({abs(f1_macro_pct):.1f}% improvement)\")\n",
    "    print(f\"   This suggests 'crash' may be causing overfitting or the model is\")\n",
    "    print(f\"   learning from other correlated features\")\n",
    "    \n",
    "else:  # No meaningful change\n",
    "    print(f\"\\nâ“ INCONCLUSIVE:\")\n",
    "    print(f\"   Removing 'crash' had minimal impact ({abs(f1_macro_pct):.1f}% change)\")\n",
    "    print(f\"   Model may be relying on correlated features like 'hang', 'freeze', etc.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ABLATION STUDY COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save quick summary\n",
    "summary = f\"\"\"\n",
    "ABLATION STUDY SUMMARY\n",
    "======================\n",
    "\n",
    "Hypothesis: Removing 'crash' (most important feature) should degrade performance\n",
    "\n",
    "Results:\n",
    "- Baseline F1-Macro:    {baseline_f1_macro:.4f}\n",
    "- Ablation F1-Macro:    {ablation_f1_macro:.4f}\n",
    "- Difference:           {f1_macro_diff:+.4f} ({f1_macro_pct:+.2f}%)\n",
    "\n",
    "- Baseline F1-Weighted: {baseline_f1_weighted:.4f}\n",
    "- Ablation F1-Weighted: {ablation_f1_weighted:.4f}\n",
    "- Difference:           {f1_weighted_diff:+.4f} ({f1_weighted_pct:+.2f}%)\n",
    "\n",
    "Conclusion: {'CONFIRMED - crash is critical' if f1_macro_diff > 0.01 else 'INCONCLUSIVE or UNEXPECTED'}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + summary)\n",
    "\n",
    "with open('../results/ablation_study_quick_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"ðŸ“„ Summary saved: ../results/ablation_study_quick_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
